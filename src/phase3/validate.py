from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple
import re
import json

import pandas as pd
import networkx as nx

from .utils import parse_tweet_ids, ensure_parent  # reuse robust parsing + safe dirs


@dataclass
class DuplicateReport:
    news_id_duplicates: int
    per_article_tweet_dupes: int
    global_tweet_dupes: int
    details_sample: Dict[str, List[str]]  # news_id -> duplicated tweet_ids sample


@dataclass
class TimestampReport:
    has_any_timestamps: bool
    sources_scanned: List[str]
    matched_columns: Dict[str, List[str]]  # file -> timestamp-like cols


@dataclass
class IsolateReport:
    bipartite_articles_isolates: int
    bipartite_tweets_isolates: int
    bipartite_edges: int
    bipartite_degree_mismatch: int  # articles whose degree != tweet_count
    similarity_isolates: int
    similarity_edges: int
    similarity_selfloops: int
    similarity_bad_weights: int  # weights not in [0,1]


@dataclass
class ValidationReport:
    duplicates: DuplicateReport
    timestamps: TimestampReport
    isolates: IsolateReport

    def to_markdown(self) -> str:
        """Render a concise validation report for PR review."""
        lines = []
        lines.append("# Phase 3 – Validation Report\n")
        lines.append("## Duplicates\n")
        lines.append(f"- Duplicate `news_id` rows in `news_clean.csv`: **{self.duplicates.news_id_duplicates}**")
        lines.append(f"- Articles with duplicated `tweet_ids` inside a row: **{self.duplicates.per_article_tweet_dupes}**")
        lines.append(f"- Global duplicate `tweet_ids` across different articles: **{self.duplicates.global_tweet_dupes}**")
        if self.duplicates.details_sample:
            js = json.dumps(self.duplicates.details_sample, indent=2)[:2000]
            lines.append("<details><summary>Examples</summary>\n\n```json\n" + js + "\n```\n</details>")
        lines.append("\n## Timestamps\n")
        if self.timestamps.has_any_timestamps:
            lines.append("- Time columns **found** in the following files:")
            for f, cols in self.timestamps.matched_columns.items():
                lines.append(f"  - `{f}` → {cols}")
        else:
            lines.append("- **No timestamp columns found** in available CSVs. "
                         "This confirms full cascades are not possible with current data.")
        lines.append("\n## Graph Isolates & Consistency\n")
        lines.append(f"- Bipartite: edges={self.isolates.bipartite_edges}, "
                     f"isolated articles={self.isolates.bipartite_articles_isolates}, "
                     f"isolated tweets={self.isolates.bipartite_tweets_isolates}, "
                     f"degree mismatches (A vs tweet_count)={self.isolates.bipartite_degree_mismatch}")
        lines.append(f"- Similarity: edges={self.isolates.similarity_edges}, "
                     f"isolates={self.isolates.similarity_isolates}, "
                     f"self-loops={self.isolates.similarity_selfloops}, "
                     f"bad weights(out of [0,1])={self.isolates.similarity_bad_weights}")
        lines.append("\n---\nGenerated by `src/phase3/validate.py`.")
        return "\n".join(lines)


# ---------- helper functions ----------

_TIME_COL_REGEX = re.compile(r"(time|timestamp|created_at|datetime|date)$", re.I)


def _list_raw_files(raw_root: Path) -> List[Path]:
    return sorted(p for p in raw_root.rglob("*.csv"))


def _news_dupe_check(news_clean: Path) -> int:
    df = pd.read_csv(news_clean, low_memory=False)
    if "news_id" not in df.columns:
        raise KeyError("Expected 'news_id' in news_clean.csv")
    return int(df["news_id"].duplicated().sum())


def _tweet_dupes(raw_files: List[Path]) -> Tuple[int, int, Dict[str, List[str]]]:
    per_row_dupe_articles = 0
    global_seen: Dict[str, List[str]] = {}  # tid -> [news_id,...]
    sample: Dict[str, List[str]] = {}
    for f in raw_files:
        try:
            df = pd.read_csv(f, low_memory=False)
        except Exception:
            continue
        nid_col = "news_id" if "news_id" in df.columns else ("id" if "id" in df.columns else None)
        if nid_col is None:
            df = df.reset_index().rename(columns={"index": "news_id"})
            nid_col = "news_id"
        if "tweet_ids" not in df.columns:
            continue
        for _, row in df[[nid_col, "tweet_ids"]].iterrows():
            nid = str(row[nid_col])
            tids = parse_tweet_ids(row["tweet_ids"])
            if not tids:
                continue
            # per-row duplicate check
            if len(tids) != len(set(tids)):
                per_row_dupe_articles += 1
                if len(sample) < 5:
                    dups = sorted(list({t for t in tids if tids.count(t) > 1}))[:10]
                    sample[nid] = dups
            # global duplicate check
            for t in set(tids):
                global_seen.setdefault(t, []).append(nid)
    global_dupes = sum(1 for v in global_seen.values() if len(set(v)) > 1)
    return per_row_dupe_articles, global_dupes, sample


def _scan_timestamp_cols(files: List[Path]) -> TimestampReport:
    matched: Dict[str, List[str]] = {}
    for f in files:
        try:
            df = pd.read_csv(f, nrows=50)
        except Exception:
            continue
        cols = [c for c in df.columns if _TIME_COL_REGEX.search(c)]
        if cols:
            matched[str(f)] = cols
    return TimestampReport(has_any_timestamps=bool(matched),
                           sources_scanned=[str(p) for p in files],
                           matched_columns=matched)


def _load_graph(graph_path: Path) -> nx.Graph:
    if not graph_path.exists():
        raise FileNotFoundError(f"Graph file not found: {graph_path}")
    return nx.read_graphml(graph_path)


def _bipartite_isolates_and_mismatch(bip_path: Path, metrics_csv: Path) -> Tuple[int, int, int, int]:
    G = _load_graph(bip_path)
    a_nodes = [n for n in G.nodes if str(n).startswith("A_")]
    t_nodes = [n for n in G.nodes if str(n).startswith("T_")]
    a_iso = sum(1 for n in a_nodes if G.degree(n) == 0)
    t_iso = sum(1 for n in t_nodes if G.degree(n) == 0)
    edges = G.number_of_edges()

    metrics = pd.read_csv(metrics_csv, low_memory=False)
    if "news_id" not in metrics.columns or "tweet_count" not in metrics.columns:
        raise KeyError("Expected 'news_id' and 'tweet_count' in article_metrics.csv")
    metrics_map = {f"A_{row['news_id']}": int(row["tweet_count"]) for _, row in metrics.iterrows()}

    mismatch = 0
    for a in a_nodes:
        if metrics_map.get(a, 0) != G.degree(a):
            mismatch += 1
    return a_iso, t_iso, edges, mismatch


def _similarity_isolates_selfloops_badweights(sim_path: Path) -> Tuple[int, int, int, int]:
    G = _load_graph(sim_path)
    iso = sum(1 for n in G.nodes if G.degree(n) == 0)
    loops = nx.number_of_selfloops(G)
    bad_w = 0
    for _, _, d in G.edges(data=True):
        w = d.get("weight", 1.0)
        if not isinstance(w, (int, float)) or w < 0.0 or w > 1.0:
            bad_w += 1
    return iso, G.number_of_edges(), loops, bad_w


# ---------- public API ----------

def run_validation(
    project_root: Path,
    news_clean_csv: Path,
    raw_root: Path,
    metrics_csv: Path,
    bipartite_graphml: Path,
    similarity_graphml: Path,
    out_report_md: Path,
) -> ValidationReport:
    """Run Phase-3 validation and write a Markdown report."""
    news_dupes = _news_dupe_check(news_clean_csv)
    raw_files = _list_raw_files(raw_root)
    per_row_dupes, global_dupes, sample = _tweet_dupes(raw_files)

    dup_rep = DuplicateReport(
        news_id_duplicates=news_dupes,
        per_article_tweet_dupes=per_row_dupes,
        global_tweet_dupes=global_dupes,
        details_sample=sample,
    )

    ts_rep = _scan_timestamp_cols(raw_files + [news_clean_csv])

    a_iso, t_iso, bip_edges, mismatch = _bipartite_isolates_and_mismatch(bipartite_graphml, metrics_csv)
    sim_iso, sim_edges, selfloops, badw = _similarity_isolates_selfloops_badweights(similarity_graphml)

    iso_rep = IsolateReport(
        bipartite_articles_isolates=a_iso,
        bipartite_tweets_isolates=t_iso,
        bipartite_edges=bip_edges,
        bipartite_degree_mismatch=mismatch,
        similarity_isolates=sim_iso,
        similarity_edges=sim_edges,
        similarity_selfloops=selfloops,
        similarity_bad_weights=badw,
    )

    report = ValidationReport(duplicates=dup_rep, timestamps=ts_rep, isolates=iso_rep)

    ensure_parent(out_report_md)
    out_report_md.write_text(report.to_markdown(), encoding="utf-8")
    print(f"[OK] Validation report → {out_report_md}")
    return report


if __name__ == "__main__":
    import argparse

    ROOT = Path(__file__).resolve().parents[2]  # project root
    PROC = ROOT / "data" / "processed"
    RAW = ROOT / "data" / "raw" / "FakeNewsNet"
    GRAPHS = PROC / "graphs"

    p = argparse.ArgumentParser(description="Phase 3 Validation (duplicates, timestamps, isolates)")
    p.add_argument("--news", type=Path, default=PROC / "news_clean.csv")
    p.add_argument("--metrics", type=Path, default=PROC / "article_metrics.csv")
    p.add_argument("--bip", type=Path, default=GRAPHS / "article_tweet_bipartite.graphml")
    p.add_argument("--sim", type=Path, default=GRAPHS / "article_similarity.graphml")
    p.add_argument("--raw", type=Path, default=RAW)
    p.add_argument("--out", type=Path, default=ROOT / "reports" / "phase3_validation_report.md")
    args = p.parse_args()

    run_validation(
        project_root=ROOT,
        news_clean_csv=args.news,
        raw_root=args.raw,
        metrics_csv=args.metrics,
        bipartite_graphml=args.bip,
        similarity_graphml=args.sim,
        out_report_md=args.out,
    )


# File: scripts/build_phase3.py  (only the main() section updated to include validation)
def main() -> None:
    import argparse
    from pathlib import Path
    ROOT = Path(__file__).resolve().parents[1]
    RAW = ROOT / "data" / "raw" / "FakeNewsNet"
    PROC = ROOT / "data" / "processed"
    GR = PROC / "graphs"
    FIG = ROOT / "reports" / "figures"

    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["tfidf", "bert"], default="tfidf")
    ap.add_argument("--k", type=int, default=10)
    ap.add_argument("--thr", type=float, default=0.25)
    ap.add_argument("--validate", action="store_true", help="run Phase-3 validation after building graphs")
    args = ap.parse_args()

    ensure_parent(PROC / "placeholder.tmp")  # create processed/ if missing

    metrics = build_article_metrics(PROC / "news_clean.csv", RAW, PROC / "article_metrics.csv")
    bip = build_article_tweet_bipartite(PROC / "article_metrics.csv", RAW, GR / "article_tweet_bipartite.graphml")
    sim = build_similarity_graph(PROC, GR / "article_similarity.graphml", mode=args.mode, k=args.k, threshold=args.thr)

    # (optional) the existing figure code stays as is

    if args.validate:
        run_validation(
            project_root=ROOT,
            news_clean_csv=PROC / "news_clean.csv",
            raw_root=RAW,
            metrics_csv=PROC / "article_metrics.csv",
            bipartite_graphml=GR / "article_tweet_bipartite.graphml",
            similarity_graphml=GR / "article_similarity.graphml",
            out_report_md=ROOT / "reports" / "phase3_validation_report.md",
        )
